# 포트폴리오: 중고 거래 서비스의 부적절한 댓글 감지 시스템



1. 프로젝트 목적:
   중고 거래 서비스에서 부적절한 언어 사용을 감지하고 차단하여 사용자 간의 원활한 커뮤니케이션과 건전한 거래 환경을 조성하는 것을 목표로 합니다.

2. 프로젝트 배경:
   중고 거래 서비스에서는 종종 사용자 간의 언쟁이나 부적절한 언어 사용이 문제가 됩니다. 이를 해결하기 위해 욕설 및 부적절한 댓글을 자동으로 감지하는 시스템을 구축하고자 합니다.

3. 사용된 기술:
   Python
   TensorFlow
   Natural Language Processing (NLP)
   TensorFlow Lite
   Keras
   nlpaug 라이브러리

```python

import numpy as np
import pickle
import nlpaug.augmenter.word as naw
import nltk
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# NLTK 데이터 다운로드
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

# 데이터 준비
texts = [
    "This is a nice comment",
    "This is a badword comment",
    "Another nice comment",
    "Another badword here"
]
labels = [0, 1, 0, 1]  # 0: 정상, 1: 욕설

# 텍스트 증강
aug = naw.SynonymAug(aug_src='wordnet')
augmented_texts = texts.copy()
for text in texts:
    for _ in range(10):  # 각 문장에 대해 10개의 증강된 문장을 생성
        augmented_texts.append(aug.augment(text))

# 증강된 데이터셋 준비
labels = labels * 11  # 각 텍스트에 대해 증강된 데이터 포함

# 데이터셋 분할
train_texts, test_texts, train_labels, test_labels = train_test_split(augmented_texts, labels, test_size=0.2)

# 텍스트 전처리
tokenizer = Tokenizer(num_words=1000)
tokenizer.fit_on_texts(train_texts)

train_sequences = tokenizer.texts_to_sequences(train_texts)
test_sequences = tokenizer.texts_to_sequences(test_texts)

maxlen = 100
train_padded = pad_sequences(train_sequences, padding='post', maxlen=maxlen)
test_padded = pad_sequences(test_sequences, padding='post', maxlen=maxlen)

# 토크나이저 저장
with open('tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

# 전처리된 데이터 저장
with open('data.npz', 'wb') as f:
    np.savez(f, train_padded=train_padded, train_labels=train_labels, test_padded=test_padded, test_labels=test_labels)

```

``` python

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
import numpy as np

# 전처리된 데이터 로드
data = np.load('data.npz')
train_padded = data['train_padded']
train_labels = data['train_labels']
test_padded = data['test_padded']
test_labels = data['test_labels']

# 모델 생성
model = Sequential([
    Embedding(input_dim=1000, output_dim=64, input_length=100),
    LSTM(128, return_sequences=True),
    Dropout(0.5),
    LSTM(128),
    Dropout(0.5),
    Dense(64, activation='relu'),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])

# 모델 훈련
model.fit(train_padded, train_labels, epochs=50, batch_size=8, validation_data=(test_padded, test_labels))

# 모델 저장 (Native Keras 형식으로 저장)
model.save('saved_model.keras')

```

```python

import tensorflow as tf

# 모델 로드
model = tf.keras.models.load_model('saved_model.keras')

# TensorFlow Lite 모델로 변환
converter = tf.lite.TFLiteConverter.from_keras_model(model)

# 변환 옵션 설정
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
converter.experimental_new_converter = True
converter.allow_custom_ops = True

# 모델을 TensorFlow Lite 형식으로 변환
tflite_model = converter.convert()

# 변환된 모델을 저장
with open('text_classification_model.tflite', 'wb') as f:
    f.write(tflite_model)

```

4.프로젝트 보완점

데이터셋 확장:
더 많은 실제 댓글 데이터를 수집하여 모델의 성능을 향상시킬 수 있습니다. 다양한 상황에서 발생할 수 있는 부적절한 언어 사용을 포괄하기 위해 데이터셋을 확장하는 것이 필요합니다.

백엔드와 프론트엔드 결합 부족:
현재 프로젝트는 모델을 학습하고 변환하는 백엔드 부분만 구현되어 있습니다. 이를 실제 서비스에 통합하기 위해서는 프론트엔드와 백엔드를 결합하여 사용자가 직접 사용할 수 있는 완성된 애플리케이션으로 발전시켜야 합니다.
